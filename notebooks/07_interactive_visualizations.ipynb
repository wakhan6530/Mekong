{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 1: í™˜ê²½ ì„¤ì • & Earth Engine ì´ˆê¸°í™” ===\n",
    "import os, json, warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ë””ë ‰í„°ë¦¬\n",
    "NB_DIR   = Path.cwd()\n",
    "OUT_DIR  = NB_DIR / \"outputs\"\n",
    "ASSETS   = OUT_DIR / \"web_assets\"\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "ASSETS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Earth Engine\n",
    "EE_PROJECT_ID = os.environ.get(\"EE_PROJECT_ID\", \"nasa-flood\")\n",
    "EE_READY = False\n",
    "try:\n",
    "    import ee\n",
    "    try:\n",
    "        ee.Initialize(project=EE_PROJECT_ID)\n",
    "        EE_READY = True\n",
    "        print(f\"âœ… Earth Engine initialized (project='{EE_PROJECT_ID}')\")\n",
    "    except Exception:\n",
    "        print(\"ðŸ” Authenticating with Earth Engine...\")\n",
    "        ee.Authenticate()\n",
    "        ee.Initialize(project=EE_PROJECT_ID)\n",
    "        EE_READY = True\n",
    "        print(f\"âœ… Authenticated & initialized (project='{EE_PROJECT_ID}')\")\n",
    "except Exception as e:\n",
    "    print(\"âš ï¸ earthengine-api not available. Will only load cached CSV/JSON.\")\n",
    "    EE_READY = False\n",
    "\n",
    "print(f\"â° Now: {datetime.now().isoformat(timespec='seconds')}\")\n",
    "print(f\"ðŸ“ outputs: {OUT_DIR.resolve()}\")\n",
    "print(f\"ðŸ“ web_assets: {ASSETS.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 2: í”„ë¡œì íŠ¸ ì„¤ì • & ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ===\n",
    "# AOI/ìœˆë„/ìž„ê³„ê°’/ì´ë²¤íŠ¸\n",
    "if EE_READY:\n",
    "    AOI_DELTA    = ee.Geometry.Rectangle([104.30,  8.50, 106.90, 10.90], geodesic=False)\n",
    "    AOI_TONLESAP = ee.Geometry.Rectangle([103.30, 12.00, 105.20, 13.70], geodesic=False)\n",
    "\n",
    "CFG = {\n",
    "    \"YEARS\": list(range(2015, 2025)),\n",
    "    \"FLOOD_MONTHS\": (8, 9),    # Augâ€“Sep\n",
    "    \"DROUGHT_MONTHS\": (3, 4),  # Marâ€“Apr\n",
    "    \"TH_VV_DB\": -16.0,\n",
    "    \"TH_VH_DB\": -22.0,\n",
    "    \"BASELINE_YEARS\": [2005, 2006, 2007, 2008],\n",
    "    \"EVENTS\": {\n",
    "        \"JINGHONG_FLOW_CUT\": \"2019-07-15\",\n",
    "        \"XIAOWAN_ONLINE\":    \"2009-01-01\",\n",
    "        \"NUOZHADU_ONLINE\":   \"2012-01-01\",\n",
    "    },\n",
    "}\n",
    "FLOOD_M1, FLOOD_M2 = CFG[\"FLOOD_MONTHS\"]\n",
    "DRY_M1,   DRY_M2   = CFG[\"DROUGHT_MONTHS\"]\n",
    "EVENTS = {k: pd.to_datetime(v) for k, v in CFG[\"EVENTS\"].items()}\n",
    "\n",
    "# ê³µí†µ ìœ í‹¸\n",
    "def _daterange_of_year_months(year:int, m1:int, m2:int):\n",
    "    start = date(year, m1, 1)\n",
    "    end   = (date(year+1,1,1) - timedelta(days=1)) if m2==12 else (date(year,m2+1,1)-timedelta(days=1))\n",
    "    return start.isoformat(), end.isoformat()\n",
    "\n",
    "if EE_READY:\n",
    "    def s1_min(aoi, start, end, pol):\n",
    "        return (ee.ImageCollection('COPERNICUS/S1_GRD')\n",
    "                .filterBounds(aoi)\n",
    "                .filterDate(start, end)\n",
    "                .filter(ee.Filter.eq('instrumentMode','IW'))\n",
    "                .filter(ee.Filter.listContains('transmitterReceiverPolarisation', pol))\n",
    "                .select(pol).min().clip(aoi))\n",
    "\n",
    "    def classify_water(img_min, pol, threshold_db):\n",
    "        return img_min.lt(threshold_db).selfMask()\n",
    "\n",
    "    def area_km2(mask_img, aoi, scale=30, band_name='constant'):\n",
    "        area = (mask_img.multiply(ee.Image.pixelArea())\n",
    "                .reduceRegion(ee.Reducer.sum(), aoi, scale, maxPixels=1e12))\n",
    "        return ee.Number(area.get(band_name)).divide(1e6)\n",
    "\n",
    "    def chirps_sum_mm(aoi, start, end):\n",
    "        col = (ee.ImageCollection('UCSB-CHG/CHIRPS/DAILY')\n",
    "               .filterBounds(aoi).filterDate(start, end).select('precipitation'))\n",
    "        # ë¹ˆ ì»¬ë ‰ì…˜ ë°©ì–´\n",
    "        if col.size().getInfo() == 0:\n",
    "            return None\n",
    "        total = col.sum().reduceRegion(ee.Reducer.mean(), aoi, 5000, maxPixels=1e12)\n",
    "        return ee.Number(total.get('precipitation'))\n",
    "\n",
    "print(\"âœ… Config ready | Years:\", CFG[\"YEARS\"][0], \"-\", CFG[\"YEARS\"][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3: ë°ì´í„° ë¡œë”© (Smart Cache System) ===\n",
    "# íŒŒì¼ ìš°ì„ , ì—†ìœ¼ë©´ EEë¡œ ê³„ì‚° (ê°€ëŠ¥í•  ë•Œë§Œ). ëª¨ë“  ê²½ë¡œëŠ” candidates ì²´ê³„ë¡œ íƒìƒ‰.\n",
    "\n",
    "# ê²½ë¡œ í›„ë³´ ìƒì„±ê¸°\n",
    "PROJECT_ROOT = NB_DIR  # ë…¸íŠ¸ë¶ ë£¨íŠ¸ ê¸°ì¤€\n",
    "CANDIDATE_DIRS = [\n",
    "    ASSETS,                                  # 07 ìºì‹œ\n",
    "    OUT_DIR,                                 # notebooks/outputs\n",
    "    PROJECT_ROOT / \"outputs\",                # í”„ë¡œì íŠ¸ ë£¨íŠ¸ outputs\n",
    "    PROJECT_ROOT / \"streamlit_app\" / \"data\" / \"processed\",  # ì•± ë°ì´í„°\n",
    "]\n",
    "\n",
    "def _first_existing(*paths: Path):\n",
    "    for p in paths:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def read_csv_candidates(*names):\n",
    "    # names: íŒŒì¼ëª… ë¬¸ìžì—´ë“¤\n",
    "    paths = [d / n for d in CANDIDATE_DIRS for n in names]\n",
    "    for p in paths:\n",
    "        if p.exists():\n",
    "            print(f\"[LOAD] CSV -> {p}\")\n",
    "            return pd.read_csv(p)\n",
    "    print(f\"[WARN] CSV not found in candidates: {[str(p) for p in paths][:3]} ...\")\n",
    "    return None\n",
    "\n",
    "def read_json_candidates(*names):\n",
    "    paths = [d / n for d in CANDIDATE_DIRS for n in names]\n",
    "    for p in paths:\n",
    "        if p.exists():\n",
    "            print(f\"[LOAD] JSON -> {p}\")\n",
    "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "    print(f\"[WARN] JSON not found in candidates: {[str(p) for p in paths][:3]} ...\")\n",
    "    return None\n",
    "\n",
    "# EE ê¸°ë°˜ ë¹Œë” (í•„ìš”í•  ë•Œë§Œ í˜¸ì¶œ)\n",
    "def _build_annual_flood_df(aoi, years):\n",
    "    rows=[]\n",
    "    for y in years:\n",
    "        start,end = _daterange_of_year_months(y, FLOOD_M1, FLOOD_M2)\n",
    "        vv = s1_min(aoi, start, end, \"VV\")\n",
    "        vh = s1_min(aoi, start, end, \"VH\")\n",
    "        vv_mask = classify_water(vv, \"VV\", CFG[\"TH_VV_DB\"])\n",
    "        vh_mask = classify_water(vh, \"VH\", CFG[\"TH_VH_DB\"])\n",
    "        a_vv = float(area_km2(vv_mask, aoi, 30).getInfo() or 0.0)\n",
    "        a_vh = float(area_km2(vh_mask, aoi, 30).getInfo() or 0.0)\n",
    "        pr_n = chirps_sum_mm(aoi, start, end)\n",
    "        pr   = float(pr_n.getInfo() or 0.0) if pr_n else 0.0\n",
    "        rows.append({\"year\":y, \"flood_vv_km2\":a_vv, \"flood_vh_km2\":a_vh, \"precip_wet_mm\":pr})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def _build_annual_dry_df(aoi, years):\n",
    "    rows=[]\n",
    "    for y in years:\n",
    "        start,end = _daterange_of_year_months(y, DRY_M1, DRY_M2)\n",
    "        vh = s1_min(aoi, start, end, \"VH\")\n",
    "        vh_mask = classify_water(vh, \"VH\", CFG[\"TH_VH_DB\"])\n",
    "        a_vh = float(area_km2(vh_mask, aoi, 30).getInfo() or 0.0)\n",
    "        pr_n = chirps_sum_mm(aoi, start, end)\n",
    "        pr   = float(pr_n.getInfo() or 0.0) if pr_n else 0.0\n",
    "        rows.append({\"year\":y, \"dry_vh_km2\":a_vh, \"precip_dry_mm\":pr})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def smart_load_flood(aoi_name: str):\n",
    "    # ì´ë¦„ ê´€ë¡€\n",
    "    lower = aoi_name.lower()\n",
    "    # 1) ìºì‹œ/ê¸°ì¡´ ì‚°ì¶œë¬¼\n",
    "    df = read_csv_candidates(\n",
    "        f\"annual_flood_{lower}.csv\",                     # 07 ìºì‹œ\n",
    "        f\"flood_extent_{lower}_vv_vh_2015_2024.csv\",     # 04 ì‚°ì¶œë¬¼ ê°€ì •\n",
    "        f\"annual_analysis_{lower}.csv\",                  # 02 ì‚°ì¶œë¬¼ ê°€ì •\n",
    "    )\n",
    "    if df is not None:\n",
    "        # ì»¬ëŸ¼ í‘œì¤€í™”\n",
    "        rename = {\n",
    "            \"vv_km2\": \"flood_vv_km2\",\n",
    "            \"vh_km2\": \"flood_vh_km2\",\n",
    "            \"precip_mm\": \"precip_wet_mm\",\n",
    "            \"precipitation_mm\": \"precip_wet_mm\",\n",
    "        }\n",
    "        df = df.rename(columns=rename)\n",
    "        need = [\"year\",\"flood_vv_km2\",\"flood_vh_km2\",\"precip_wet_mm\"]\n",
    "        # AOI ì—´ì´ ìžˆìœ¼ë©´ í•„í„°\n",
    "        if \"aoi\" in df.columns:\n",
    "            df = df[df[\"aoi\"].str.lower().str.replace(\" \",\"_\")==lower]\n",
    "        # ê²°ì¸¡ ì—´ ë³´ì™„\n",
    "        for c in need:\n",
    "            if c not in df.columns:\n",
    "                df[c] = np.nan\n",
    "        df = df[need].drop_duplicates(subset=\"year\").sort_values(\"year\")\n",
    "        # ìºì‹œë¡œ ì €ìž¥\n",
    "        (ASSETS / f\"annual_flood_{lower}.csv\").write_text(df.to_csv(index=False), encoding=\"utf-8\")\n",
    "        return df\n",
    "\n",
    "    # 2) EEë¡œ ê³„ì‚° (ê°€ëŠ¥í•  ë•Œë§Œ)\n",
    "    if EE_READY:\n",
    "        print(f\"[BUILD] Computing flood (VV/VH) for {aoi_name} via GEE â€¦\")\n",
    "        aoi = AOI_DELTA if aoi_name==\"Mekong_Delta\" else AOI_TONLESAP\n",
    "        df = _build_annual_flood_df(aoi, CFG[\"YEARS\"])\n",
    "        df.to_csv(ASSETS / f\"annual_flood_{lower}.csv\", index=False)\n",
    "        return df\n",
    "\n",
    "    print(f\"[ERROR] No flood dataset & EE not available â†’ {aoi_name}\")\n",
    "    return pd.DataFrame(columns=[\"year\",\"flood_vv_km2\",\"flood_vh_km2\",\"precip_wet_mm\"])\n",
    "\n",
    "def smart_load_dry(aoi_name: str):\n",
    "    lower = aoi_name.lower()\n",
    "    df = read_csv_candidates(\n",
    "        f\"annual_dry_{lower}.csv\",                 # 07 ìºì‹œ\n",
    "        \"dry_season_analysis_2015_2024.csv\",       # 06 ì‚°ì¶œë¬¼\n",
    "    )\n",
    "    if df is not None:\n",
    "        if \"aoi\" in df.columns:\n",
    "            df = df[df[\"aoi\"].str.lower().str.replace(\" \",\"_\")==lower].copy()\n",
    "        rename = {\n",
    "            \"water_extent_km2\": \"dry_vh_km2\",\n",
    "            \"precip_total_mm\":  \"precip_dry_mm\",\n",
    "        }\n",
    "        df = df.rename(columns=rename)\n",
    "        need = [\"year\",\"dry_vh_km2\",\"precip_dry_mm\"]\n",
    "        for c in need:\n",
    "            if c not in df.columns:\n",
    "                df[c] = np.nan\n",
    "        df = df[need].drop_duplicates(subset=\"year\").sort_values(\"year\")\n",
    "        (ASSETS / f\"annual_dry_{lower}.csv\").write_text(df.to_csv(index=False), encoding=\"utf-8\")\n",
    "        return df\n",
    "\n",
    "    if EE_READY:\n",
    "        print(f\"[BUILD] Computing dry-season (VH) for {aoi_name} via GEE â€¦\")\n",
    "        aoi = AOI_DELTA if aoi_name==\"Mekong_Delta\" else AOI_TONLESAP\n",
    "        df = _build_annual_dry_df(aoi, CFG[\"YEARS\"])\n",
    "        df.to_csv(ASSETS / f\"annual_dry_{lower}.csv\", index=False)\n",
    "        return df\n",
    "\n",
    "    print(f\"[ERROR] No dry dataset & EE not available â†’ {aoi_name}\")\n",
    "    return pd.DataFrame(columns=[\"year\",\"dry_vh_km2\",\"precip_dry_mm\"])\n",
    "\n",
    "# ì‹¤ì œ ë¡œë“œ\n",
    "print(\"=\"*80, \"\\nðŸ“Š LOADING (smart cache)\\n\", \"=\"*80, sep=\"\")\n",
    "df_delta_flood = smart_load_flood(\"Mekong_Delta\")\n",
    "df_ts_flood    = smart_load_flood(\"Tonle_Sap\")\n",
    "df_delta_dry   = smart_load_dry(\"Mekong_Delta\")\n",
    "df_ts_dry      = smart_load_dry(\"Tonle_Sap\")\n",
    "\n",
    "# ë¨¸ì§€ ë° íŒŒìƒì§€í‘œ\n",
    "df_delta = pd.merge(df_delta_flood, df_delta_dry, on=\"year\", how=\"outer\").sort_values(\"year\")\n",
    "df_ts    = pd.merge(df_ts_flood,    df_ts_dry,    on=\"year\", how=\"outer\").sort_values(\"year\")\n",
    "for df in (df_delta, df_ts):\n",
    "    if {\"flood_vh_km2\",\"flood_vv_km2\"}.issubset(df.columns):\n",
    "        df[\"vh_gain_km2\"] = df[\"flood_vh_km2\"] - df[\"flood_vv_km2\"]\n",
    "\n",
    "print(f\"Delta rows: {len(df_delta)} | TonlÃ© rows: {len(df_ts)}\")\n",
    "display(df_delta.head(3))\n",
    "display(df_ts.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4: Baseline ë¡œë”© (Notebook 01 ìž¬ì‚¬ìš©) ===\n",
    "def load_or_fallback_baselines():\n",
    "    b = read_json_candidates(\"baseline_summary.json\")\n",
    "    if b and \"areas\" in b:\n",
    "        base = {}\n",
    "        for a in b[\"areas\"]:\n",
    "            base[a[\"aoi\"]] = {\n",
    "                \"wet_km2\": float(a.get(\"baseline_wet_km2\", 0.0)),\n",
    "                \"dry_km2\": float(a.get(\"baseline_dry_km2\", 0.0)),\n",
    "            }\n",
    "        print(\"âœ… Baseline loaded from baseline_summary.json\")\n",
    "        return base\n",
    "    print(\"âš ï¸  baseline_summary.json not found â†’ using rough fallback values\")\n",
    "    return {\n",
    "        \"Mekong_Delta\": {\"wet_km2\": 8500.0, \"dry_km2\": 3200.0},\n",
    "        \"Tonle_Sap\":    {\"wet_km2\": 12000.0, \"dry_km2\": 2800.0},\n",
    "    }\n",
    "\n",
    "BASE = load_or_fallback_baselines()\n",
    "BASE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5: ë°ì´í„° ê²€ì¦ & í’ˆì§ˆ ì²´í¬ ===\n",
    "def validate_data(df: pd.DataFrame, label: str):\n",
    "    issues = []\n",
    "    # 1) ì—°ë„ ê²°ì¸¡\n",
    "    missing = sorted(set(CFG[\"YEARS\"]) - set(df[\"year\"].dropna().astype(int).tolist()))\n",
    "    if missing:\n",
    "        issues.append(f\"Missing years: {missing}\")\n",
    "    # 2) NaN\n",
    "    nan_cols = df.columns[df.isna().any()].tolist()\n",
    "    if nan_cols:\n",
    "        issues.append(f\"NaN present in: {nan_cols}\")\n",
    "    # 3) ê°’ ë²”ìœ„\n",
    "    if \"flood_vv_km2\" in df.columns and df[\"flood_vv_km2\"].notna().any():\n",
    "        if df[\"flood_vv_km2\"].max() > 60000: issues.append(\"VV too large (>60k kmÂ²)\")\n",
    "        if (df[\"flood_vv_km2\"] < 0).any():   issues.append(\"VV negative\")\n",
    "    if \"flood_vh_km2\" in df.columns and df[\"flood_vh_km2\"].notna().any():\n",
    "        if df[\"flood_vh_km2\"].max() > 70000: issues.append(\"VH too large (>70k kmÂ²)\")\n",
    "        if (df[\"flood_vh_km2\"] < 0).any():   issues.append(\"VH negative\")\n",
    "    # 4) VH â‰¥ VV ìœ„ë°°(ìžˆì„ ìˆ˜ ìžˆì§€ë§Œ ê²½ê³ )\n",
    "    if {\"flood_vv_km2\",\"flood_vh_km2\"}.issubset(df.columns):\n",
    "        viol = df[(df[\"flood_vh_km2\"].notna()) & (df[\"flood_vv_km2\"].notna()) & (df[\"flood_vh_km2\"] < df[\"flood_vv_km2\"])]\n",
    "        if len(viol) > 0:\n",
    "            issues.append(f\"VH<VV years: {viol['year'].astype(int).tolist()} (check wind/roughness)\")\n",
    "    # 5) ê°•ìˆ˜ ê²€ì¦\n",
    "    if \"precip_wet_mm\" in df.columns and df[\"precip_wet_mm\"].notna().any():\n",
    "        if df[\"precip_wet_mm\"].max() > 2000:\n",
    "            issues.append(\"Wet precip extreme (>2000 mm in 2 months)\")\n",
    "\n",
    "    if issues:\n",
    "        print(f\"âš ï¸  [{label}] Issues:\")\n",
    "        for it in issues: print(\"  â€¢\", it)\n",
    "        return False\n",
    "    print(f\"âœ… [{label}] checks passed\")\n",
    "    return True\n",
    "\n",
    "ok_delta = validate_data(df_delta, \"Mekong Delta\")\n",
    "ok_ts    = validate_data(df_ts,    \"TonlÃ© Sap\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 6: Plotting ìœ í‹¸ë¦¬í‹° (ìž¬ì‚¬ìš© ê°€ëŠ¥) ===\n",
    "COLORS = {\n",
    "    \"vv\": \"#1f77b4\", \"vh\": \"#ff7f0e\", \"precip\": \"#2ca02c\",\n",
    "    \"baseline\": \"#d62728\", \"event\": \"#7f7f7f\", \"gain\": \"#9467bd\"\n",
    "}\n",
    "\n",
    "def create_dual_subplot(title_left, title_right, height=500, width=1200):\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=(title_left, title_right), horizontal_spacing=0.10)\n",
    "    fig.update_layout(height=height, width=width, hovermode=\"x unified\",\n",
    "                      font=dict(family=\"Arial, sans-serif\", size=12),\n",
    "                      plot_bgcolor=\"white\", paper_bgcolor=\"white\", showlegend=True)\n",
    "    fig.update_xaxes(dtick=1, gridcolor=\"lightgray\", showline=True, linewidth=1, linecolor=\"black\", mirror=True)\n",
    "    fig.update_yaxes(gridcolor=\"lightgray\", showline=True, linewidth=1, linecolor=\"black\", mirror=True)\n",
    "    return fig\n",
    "\n",
    "def finalize_figure(fig, title, save_json: Path=None):\n",
    "    fig.update_layout(title=dict(text=title, x=0.5, xanchor=\"center\", font=dict(size=16)))\n",
    "    if save_json is not None:\n",
    "        save_json.parent.mkdir(parents=True, exist_ok=True)\n",
    "        save_json.write_text(fig.to_json(), encoding=\"utf-8\")\n",
    "        print(f\"ðŸ’¾ Saved JSON -> {save_json}\")\n",
    "    return fig\n",
    "\n",
    "def add_event_markers(fig, events_dict, row=1, col=1):\n",
    "    for label, ts in events_dict.items():\n",
    "        year = ts.year\n",
    "        fig.add_vline(x=year, line_dash=\"dot\", line_color=COLORS[\"event\"], opacity=0.6,\n",
    "                      annotation_text=str(year), annotation_position=\"top left\", row=row, col=col)\n",
    "\n",
    "def add_baseline(fig, value, text, row=1, col=1, color=COLORS[\"baseline\"]):\n",
    "    fig.add_hline(y=value, line_dash=\"dash\", line_color=color, opacity=0.7,\n",
    "                  annotation_text=f\"{text}: {value:,.0f} kmÂ²\", annotation_position=\"top left\",\n",
    "                  row=row, col=col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 7: Figure 1 - Annual Flood Extent ===\n",
    "def fig_annual_flood(df_delta, df_ts, baselines, events):\n",
    "    fig = create_dual_subplot(\"Mekong Delta â€” Flood Season (Augâ€“Sep)\",\n",
    "                              \"TonlÃ© Sap â€” Flood Season (Augâ€“Sep)\",\n",
    "                              height=520, width=1200)\n",
    "    # Delta\n",
    "    if {\"year\",\"flood_vv_km2\",\"flood_vh_km2\"}.issubset(df_delta.columns) and len(df_delta):\n",
    "        fig.add_trace(go.Scatter(x=df_delta[\"year\"], y=df_delta[\"flood_vv_km2\"],\n",
    "                                 mode=\"lines+markers\", name=\"Delta VV\",\n",
    "                                 line=dict(color=COLORS[\"vv\"], width=2.5)), row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(x=df_delta[\"year\"], y=df_delta[\"flood_vh_km2\"],\n",
    "                                 mode=\"lines+markers\", name=\"Delta VH\",\n",
    "                                 line=dict(color=COLORS[\"vh\"], width=2.5)), row=1, col=1)\n",
    "        add_baseline(fig, baselines[\"Mekong_Delta\"][\"wet_km2\"], \"Pre-dam wet baseline\", row=1, col=1)\n",
    "        add_event_markers(fig, events, row=1, col=1)\n",
    "    else:\n",
    "        fig.add_annotation(text=\"No data\", xref=\"paper\", yref=\"paper\", x=0.5, y=0.5, showarrow=False, row=1, col=1)\n",
    "\n",
    "    # TonlÃ©\n",
    "    if {\"year\",\"flood_vv_km2\",\"flood_vh_km2\"}.issubset(df_ts.columns) and len(df_ts):\n",
    "        fig.add_trace(go.Scatter(x=df_ts[\"year\"], y=df_ts[\"flood_vv_km2\"],\n",
    "                                 mode=\"lines+markers\", name=\"TonlÃ© VV\",\n",
    "                                 line=dict(color=COLORS[\"vv\"], width=2.5)), row=1, col=2)\n",
    "        fig.add_trace(go.Scatter(x=df_ts[\"year\"], y=df_ts[\"flood_vh_km2\"],\n",
    "                                 mode=\"lines+markers\", name=\"TonlÃ© VH\",\n",
    "                                 line=dict(color=COLORS[\"vh\"], width=2.5)), row=1, col=2)\n",
    "        add_baseline(fig, baselines[\"Tonle_Sap\"][\"wet_km2\"], \"Pre-dam wet baseline\", row=1, col=2)\n",
    "        add_event_markers(fig, events, row=1, col=2)\n",
    "    else:\n",
    "        fig.add_annotation(text=\"No data\", xref=\"paper\", yref=\"paper\", x=0.5, y=0.5, showarrow=False, row=1, col=2)\n",
    "\n",
    "    fig.update_yaxes(title_text=\"Flood Area (kmÂ²)\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Flood Area (kmÂ²)\", row=1, col=2)\n",
    "    return finalize_figure(fig, \"Annual Flood Extent (VV vs VH, 2015â€“2024)\",\n",
    "                           ASSETS / \"fig_annual_flood.json\")\n",
    "\n",
    "fig1 = fig_annual_flood(df_delta, df_ts, BASE, EVENTS)\n",
    "fig1.show()\n",
    "\n",
    "# í•µì‹¬ ì§€í‘œ\n",
    "def _avg_gain_pct(df):\n",
    "    if {\"vh_gain_km2\",\"flood_vv_km2\"}.issubset(df.columns):\n",
    "        s = df.replace(0, np.nan)\n",
    "        return float((s[\"vh_gain_km2\"]/s[\"flood_vv_km2\"]*100).dropna().mean() or 0.0)\n",
    "    return np.nan\n",
    "\n",
    "print(\"ðŸ’¡ VH extra detection (avg over years):\")\n",
    "print(f\" - Delta : { _avg_gain_pct(df_delta):.1f}%\")\n",
    "print(f\" - TonlÃ© : { _avg_gain_pct(df_ts):.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 8: Figure 2 - Flood vs Precipitation ===\n",
    "def fig_flood_vs_precip(df_delta, df_ts):\n",
    "    fig = create_dual_subplot(\"Mekong Delta â€” Flood vs Precip (Augâ€“Sep)\",\n",
    "                              \"TonlÃ© Sap â€” Flood vs Precip (Augâ€“Sep)\", height=520, width=1200)\n",
    "    # Delta\n",
    "    if {\"precip_wet_mm\",\"flood_vh_km2\"}.issubset(df_delta.columns) and len(df_delta.dropna()):\n",
    "        d = df_delta.dropna(subset=[\"precip_wet_mm\",\"flood_vh_km2\"]).copy()\n",
    "        fig.add_trace(go.Scatter(x=d[\"precip_wet_mm\"], y=d[\"flood_vh_km2\"],\n",
    "                                 mode=\"markers+text\", text=d[\"year\"].astype(int),\n",
    "                                 textposition=\"top center\", name=\"Delta\",\n",
    "                                 marker=dict(size=10, color=d[\"year\"], colorscale=\"Viridis\", showscale=True)),\n",
    "                      row=1, col=1)\n",
    "        if len(d)>=2:\n",
    "            x, y = d[\"precip_wet_mm\"].values, d[\"flood_vh_km2\"].values\n",
    "            coeff = np.polyfit(x,y,1); xs = np.linspace(x.min(), x.max(), 100); ys = coeff[0]*xs+coeff[1]\n",
    "            r = np.corrcoef(x,y)[0,1]\n",
    "            fig.add_trace(go.Scatter(x=xs, y=ys, mode=\"lines\", line=dict(color=\"gray\", dash=\"dash\"),\n",
    "                                     name=f\"fit (r={r:.2f})\"), row=1, col=1)\n",
    "    else:\n",
    "        fig.add_annotation(text=\"No data\", xref=\"paper\", yref=\"paper\", x=0.5, y=0.5, showarrow=False, row=1, col=1)\n",
    "\n",
    "    # TonlÃ©\n",
    "    if {\"precip_wet_mm\",\"flood_vh_km2\"}.issubset(df_ts.columns) and len(df_ts.dropna()):\n",
    "        t = df_ts.dropna(subset=[\"precip_wet_mm\",\"flood_vh_km2\"]).copy()\n",
    "        fig.add_trace(go.Scatter(x=t[\"precip_wet_mm\"], y=t[\"flood_vh_km2\"],\n",
    "                                 mode=\"markers+text\", text=t[\"year\"].astype(int),\n",
    "                                 textposition=\"top center\", name=\"TonlÃ©\",\n",
    "                                 marker=dict(size=10, color=t[\"year\"], colorscale=\"Plasma\", showscale=True)),\n",
    "                      row=1, col=2)\n",
    "        if len(t)>=2:\n",
    "            x, y = t[\"precip_wet_mm\"].values, t[\"flood_vh_km2\"].values\n",
    "            coeff = np.polyfit(x,y,1); xs = np.linspace(x.min(), x.max(), 100); ys = coeff[0]*xs+coeff[1]\n",
    "            r = np.corrcoef(x,y)[0,1]\n",
    "            fig.add_trace(go.Scatter(x=xs, y=ys, mode=\"lines\", line=dict(color=\"gray\", dash=\"dash\"),\n",
    "                                     name=f\"fit (r={r:.2f})\"), row=1, col=2)\n",
    "    else:\n",
    "        fig.add_annotation(text=\"No data\", xref=\"paper\", yref=\"paper\", x=0.5, y=0.5, showarrow=False, row=1, col=2)\n",
    "\n",
    "    fig.update_xaxes(title_text=\"Precipitation (mm)\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Precipitation (mm)\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Flood (VH, kmÂ²)\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Flood (VH, kmÂ²)\", row=1, col=2)\n",
    "    return finalize_figure(fig, \"Flood vs Precipitation (Augâ€“Sep)\", ASSETS / \"fig_flood_vs_precip.json\")\n",
    "\n",
    "fig2 = fig_flood_vs_precip(df_delta, df_ts)\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 9: Figure 3 - Dry Season Analysis (Dual Axis) ===\n",
    "def fig_dry_dual_axis(df_delta, df_ts, baselines, events):\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Mekong Delta â€” Dry Season\", \"TonlÃ© Sap â€” Dry Season\"),\n",
    "                        specs=[[{\"secondary_y\": True},{\"secondary_y\": True}]], horizontal_spacing=0.10,\n",
    "                        )\n",
    "    fig.update_layout(height=520, width=1200, hovermode=\"x unified\",\n",
    "                      font=dict(family=\"Arial, sans-serif\", size=12), plot_bgcolor=\"white\", paper_bgcolor=\"white\")\n",
    "\n",
    "    # Delta\n",
    "    if {\"year\",\"dry_vh_km2\",\"precip_dry_mm\"}.issubset(df_delta.columns) and len(df_delta):\n",
    "        fig.add_trace(go.Bar(x=df_delta[\"year\"], y=df_delta[\"dry_vh_km2\"], name=\"Dry water (kmÂ²)\"),\n",
    "                      row=1, col=1, secondary_y=False)\n",
    "        fig.add_trace(go.Scatter(x=df_delta[\"year\"], y=df_delta[\"precip_dry_mm\"], mode=\"lines+markers\",\n",
    "                                 name=\"Dry precip (mm)\", line=dict(color=COLORS[\"precip\"])),\n",
    "                      row=1, col=1, secondary_y=True)\n",
    "        add_baseline(fig, baselines[\"Mekong_Delta\"][\"dry_km2\"], \"Pre-dam dry baseline\", row=1, col=1)\n",
    "        add_event_markers(fig, events, row=1, col=1)\n",
    "    else:\n",
    "        fig.add_annotation(text=\"No data\", xref=\"paper\", yref=\"paper\", x=0.5, y=0.5, showarrow=False, row=1, col=1)\n",
    "\n",
    "    # TonlÃ©\n",
    "    if {\"year\",\"dry_vh_km2\",\"precip_dry_mm\"}.issubset(df_ts.columns) and len(df_ts):\n",
    "        fig.add_trace(go.Bar(x=df_ts[\"year\"], y=df_ts[\"dry_vh_km2\"], name=\"Dry water (kmÂ²)\"),\n",
    "                      row=1, col=2, secondary_y=False)\n",
    "        fig.add_trace(go.Scatter(x=df_ts[\"year\"], y=df_ts[\"precip_dry_mm\"], mode=\"lines+markers\",\n",
    "                                 name=\"Dry precip (mm)\", line=dict(color=COLORS[\"precip\"])),\n",
    "                      row=1, col=2, secondary_y=True)\n",
    "        add_baseline(fig, baselines[\"Tonle_Sap\"][\"dry_km2\"], \"Pre-dam dry baseline\", row=1, col=2)\n",
    "        add_event_markers(fig, events, row=1, col=2)\n",
    "    else:\n",
    "        fig.add_annotation(text=\"No data\", xref=\"paper\", yref=\"paper\", x=0.5, y=0.5, showarrow=False, row=1, col=2)\n",
    "\n",
    "    fig.update_yaxes(title_text=\"Water (kmÂ²)\", secondary_y=False, row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Precip (mm)\",  secondary_y=True,  row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Water (kmÂ²)\", secondary_y=False, row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Precip (mm)\",  secondary_y=True,  row=1, col=2)\n",
    "    fig.update_xaxes(dtick=1)\n",
    "\n",
    "    return finalize_figure(fig, \"Dry-season Water vs Precipitation\", ASSETS / \"fig_dry_biaxis.json\")\n",
    "\n",
    "fig3 = fig_dry_dual_axis(df_delta, df_ts, BASE, EVENTS)\n",
    "fig3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 10: Figure 4 - VH Gain Analysis ===\n",
    "def fig_vh_gain(df_delta, df_ts):\n",
    "    fig = create_dual_subplot(\"Mekong Delta â€” VH minus VV\", \"TonlÃ© Sap â€” VH minus VV\", height=480, width=1200)\n",
    "    if \"vh_gain_km2\" in df_delta.columns and len(df_delta):\n",
    "        fig.add_trace(go.Bar(x=df_delta[\"year\"], y=df_delta[\"vh_gain_km2\"], name=\"Delta VH gain\",\n",
    "                             marker_color=COLORS[\"gain\"]), row=1, col=1)\n",
    "    else:\n",
    "        fig.add_annotation(text=\"No data\", xref=\"paper\", yref=\"paper\", x=0.5, y=0.5, showarrow=False, row=1, col=1)\n",
    "    if \"vh_gain_km2\" in df_ts.columns and len(df_ts):\n",
    "        fig.add_trace(go.Bar(x=df_ts[\"year\"], y=df_ts[\"vh_gain_km2\"], name=\"TonlÃ© VH gain\",\n",
    "                             marker_color=COLORS[\"gain\"]), row=1, col=2)\n",
    "    else:\n",
    "        fig.add_annotation(text=\"No data\", xref=\"paper\", yref=\"paper\", x=0.5, y=0.5, showarrow=False, row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"VH-only additional area (kmÂ²)\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"VH-only additional area (kmÂ²)\", row=1, col=2)\n",
    "    return finalize_figure(fig, \"Additional Inundation Detected by VH (vs VV)\", ASSETS / \"fig_vh_gain.json\")\n",
    "\n",
    "fig4 = fig_vh_gain(df_delta, df_ts)\n",
    "fig4.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 11: Manifest ìƒì„± (Dashboardìš©) ===\n",
    "manifest = {\n",
    "    \"project\": \"Mekong SAR Flood Insight\",\n",
    "    \"version\": \"1.0.0\",\n",
    "    \"generated_utc\": pd.Timestamp.utcnow().isoformat(),\n",
    "    \"events\": {k:str(v.date()) for k,v in EVENTS.items()},\n",
    "    \"baselines\": BASE,\n",
    "    \"figures\": {\n",
    "        \"annual_flood\":     \"fig_annual_flood.json\",\n",
    "        \"flood_vs_precip\":  \"fig_flood_vs_precip.json\",\n",
    "        \"dry_biaxis\":       \"fig_dry_biaxis.json\",\n",
    "        \"vh_gain\":          \"fig_vh_gain.json\",\n",
    "    }\n",
    "}\n",
    "(ASSETS / \"manifest.json\").write_text(json.dumps(manifest, indent=2), encoding=\"utf-8\")\n",
    "print(\"ðŸ’¾ Saved ->\", ASSETS / \"manifest.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 12: Streamlit App ìƒì„± ===\n",
    "app_code = r\"\"\"\n",
    "import json\n",
    "from pathlib import Path\n",
    "import plotly.graph_objects as go\n",
    "import streamlit as st\n",
    "\n",
    "ASSETS = Path(\"outputs/web_assets\")\n",
    "\n",
    "st.set_page_config(page_title=\"Mekong SAR Flood Insight\", layout=\"wide\")\n",
    "st.title(\"Mekong SAR Flood Insight â€” Interactive Dashboard\")\n",
    "\n",
    "# Manifest\n",
    "mf = json.loads((ASSETS / \"manifest.json\").read_text(encoding=\"utf-8\"))\n",
    "st.caption(f\"Generated (UTC): {mf['generated_utc']}\")\n",
    "\n",
    "# Row 1\n",
    "st.subheader(\"Annual Flood Extent (VV vs VH)\")\n",
    "fig1_json = json.loads((ASSETS / mf[\"figures\"][\"annual_flood\"]).read_text(encoding=\"utf-8\"))\n",
    "st.plotly_chart(go.Figure(fig1_json), use_container_width=True)\n",
    "\n",
    "# Row 2\n",
    "col1, col2 = st.columns(2)\n",
    "with col1:\n",
    "    st.subheader(\"Flood vs Precipitation (Augâ€“Sep)\")\n",
    "    fig2_json = json.loads((ASSETS / mf[\"figures\"][\"flood_vs_precip\"]).read_text(encoding=\"utf-8\"))\n",
    "    st.plotly_chart(go.Figure(fig2_json), use_container_width=True)\n",
    "with col2:\n",
    "    st.subheader(\"Dry-season Water vs Precipitation\")\n",
    "    fig3_json = json.loads((ASSETS / mf[\"figures\"][\"dry_biaxis\"]).read_text(encoding=\"utf-8\"))\n",
    "    st.plotly_chart(go.Figure(fig3_json), use_container_width=True)\n",
    "\n",
    "# Row 3\n",
    "st.subheader(\"Additional Inundation Detected by VH (vs VV)\")\n",
    "try:\n",
    "    fig4_json = json.loads((ASSETS / mf[\"figures\"][\"vh_gain\"]).read_text(encoding=\"utf-8\"))\n",
    "    st.plotly_chart(go.Figure(fig4_json), use_container_width=True)\n",
    "except Exception:\n",
    "    st.info(\"VH gain figure not available.\")\n",
    "\n",
    "st.divider()\n",
    "st.markdown(\"**Data**: COPERNICUS/S1_GRD, CHIRPS, Landsat5 C2.  \"\n",
    "            \"**Method**: SAR thresholding + dual-polarization comparison + Landsat baselines.\")\n",
    "\"\"\"\n",
    "(NB_DIR / \"app_streamlit.py\").write_text(app_code, encoding=\"utf-8\")\n",
    "print(\"ðŸ’¾ Saved -> app_streamlit.py\")\n",
    "print(\"\\nRun locally:\\n  streamlit run app_streamlit.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
